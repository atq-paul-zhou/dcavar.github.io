<html>
<head>
<meta name="AUTHOR" content="Damir Cavar">
<meta http-equiv="Content-Language" content="en-us">
<meta name="GENERATOR" content="Microsoft FrontPage 5.0">
<meta name="ProgId" content="FrontPage.Editor.Document">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
</head>
<title>BOOT-LA - Bootstrapping in Language Acquisition - Psychological, 
Linguistic and Computational Aspects</title>
<style fprolloverstyle>A:hover {color: #FF0000; font-weight: bold}
 li.MsoNormal
	{mso-style-parent:"";
	margin-bottom:.0001pt;
	font-size:12.0pt;
	font-family:Times;
	margin-left:0in; margin-right:0in; margin-top:0in}
h5
	{margin-bottom:.0001pt;
	page-break-after:avoid;
	font-size:10.0pt;
	font-family:"Times New Roman"; margin-left:0in; margin-right:0in; margin-top:0in}
h6
	{margin-bottom:.0001pt;
	text-align:justify;
	page-break-after:avoid;
	font-size:10.0pt;
	font-family:"Times New Roman";
	text-decoration:underline;
	text-underline:single; margin-left:0in; margin-right:0in; margin-top:0in}
</style>
<h1 align="center"><a name="Top"></a><font color="#0000FF">BOOT-LA</font></h1>
<h2 align="center"><font color="#0000FF">Bootstrapping in Language Acquisition</font></h2>
<h2 align="center"><font color="#0000FF">Psychological, Linguistic and Computational Aspects</font></h2>
<p>&nbsp;</p>
<p>&nbsp;</p>
<table border="0" cellpadding="4" style="border-collapse: collapse" bordercolor="#111111" width="100%" id="AutoNumber1" bgcolor="#99CCFF">
  <tr>
    <td width="25%" align="center"><a href="#Scope">Scope</a></td>
    <td width="25%" align="center"><a href="#Location">Location</a></td>
    <td width="25%" align="center"><a href="#Time">Time</a></td>
    <td width="25%" align="center"><a href="#AreasOfInterest">Areas of Interest</a></td>
  </tr>
  <tr>
    <td width="25%" align="center"><a href="#InvitedSpeakers">Invited speakers</a></td>
    <td width="25%" align="center"><a href="#OrganisationalIssues">
    Organisational Issues</a></td>
    <td width="25%" align="center"><a href="#AbstractSubmission">Abstract 
    Submission</a></td>
    <td width="25%" align="center"><a href="#Contact">Contact</a></td>
  </tr>
  <tr>
    <td width="25%" align="center"><a href="#Program">Program</a></td>
    <td width="25%" align="center"><a href="#LSRL33">LSRL 33</a></td>
    <td width="25%" align="center"><a href="#Abstracts">Abstracts</a></td>
    <td width="25%" align="center">&nbsp;</td>
  </tr>
</table>
<p>&nbsp;</p>
<p>&nbsp;</p>
<h3><a name="Scope"></a>Scope</h3>
<p>Bootstrapping approaches to language acquisition have been extensively addressed by researchers in domains like psycholinguistics, computational and theoretical linguistics, cognitive science, machine learning.</p>
<p>This workshop aims at bringing together researchers in these fields.</p>
<p>&nbsp;</p>
<p align="right"><a href="#Top">back to TOP</a></p>
<p>&nbsp;</p>
<h3><a name="Location"></a>Location</h3>
<p>Indiana University, Bloomington campus, Bloomington, Indiana.</p>
<p>&nbsp;</p>
<p align="right"><a href="#Top">back to TOP</a></p>
<p>&nbsp;</p>
<h3><a name="Time"></a>Time</h3>
<p>21st to 23rd of April, 2003</p>
<p>&nbsp;</p>
<p align="right"><a href="#Top">back to TOP</a></p>
<p>&nbsp;</p>
<p></p>
<p></p>
<h3><a name="AreasOfInterest"></a>Areas of interest</h3>
<p>The workshop seeks to provide a forum for presentation and discussion of original research on different aspects of bootstrapping including, but not limited to:</p>
<ul>
  <li>its role in language acquisition for different domains, from a theoretical, computational and psycholinguistic point of view</li>
  <li>empirical evidence for bootstrapping strategies from psycholinguistic, psychological or cognitive science</li>
  <li>concepts, algorithms, techniques, strategies and consequences from work on:</li>
  <ul>
  	<li>grammar induction</li>
  	<li>learnability theory</li>
  	<li>parameter setting approaches</li>
  	<li>constraint-based approaches</li>
  	<li>cue-based learning</li>
  </ul>
  <li>computational approaches to learning subcategorization frames</li>
  <li>how much knowledge can be bootstrapped from function words</li>
  <li>strategies for lexical/sense disambiguation.</li>
</ul>
<p>&nbsp;</p>
<p align="right"><a href="#Top">back to TOP</a></p>
<p>&nbsp;</p>
<h3><a name="InvitedSpeakers"></a>Invited speakers are:</h3>
<ul>
  <li>Morten Christiansen (Cornell University)</li>
  <li>Michael Gasser (Indiana University)</li>
  <li>William Sakas (City University of New York)</li>
  <li>Linda Smith (Indiana University)</li>
  <li>Juan Uriagereka (University of Maryland
  <span style="font-size: 12.0pt; font-family: Times New Roman">and University 
  of the Basque Country</span>)</li>
  <li>Jürgen Weissenborn (Universität Potsdam)</li>
  <li>Charles Yang (Yale University)</li>
</ul>
<p>&nbsp;</p>
<p align="right"><a href="#Top">back to TOP</a></p>
<p>&nbsp;</p>
<h3><a name="OrganisationalIssues"></a>Organisational Issues</h3>
<h4><a name="AbstractSubmission"></a>Abstract Submission Deadline</h4>
<p><b><font color="#FF0000">Friday, March 1st, 2003</font></b></p>
<h4>How to Submit Your Abstract</h4>
<p>Abstracts can be at most 400 words of text. You may also include examples, references and data summaries (but no data charts or diagrams). This additional material, taken together, should not exceed 15 lines of text.</p>
<h4>Method of Submission</h4>
<p>As an e-mail attachment.<br>
Name, Title and affiliation should be sent in a different attachment.</p>
<p>Specify the subject as: Abstract BOOT-LA</p>
<p>Send your abstract to:</p>
<p>Damir Cavar &lt;<a href="mailto:dcavar@indiana.edu?subject=Abstract BOOT-LA: ">dcavar@indiana.edu</a>&gt;</p>
<p>and/or</p>
<p>Khaled Elghamry &lt;<a href="mailto:kelghamr@indiana.edu?subject=Abstract BOOT-LA: ">kelghamr@indiana.edu</a>&gt;</p>
<p>&nbsp;</p>
<p align="right"><a href="#Top">back to TOP</a></p>
<p>&nbsp;</p>
<h3>Organisation</h3>
<ul>
  <li>Damir Cavar</li>
  <li>Malgorzata Cavar</li>
  <li>Laurent Dekydtspotter</li>
  <li>Khaled Elghamry</li>
  <li>Steven Franks</li>
  <li>Rex Sprouse</li>
</ul>
<p>&nbsp;</p>
<p align="right"><a href="#Top">back to TOP</a></p>
<p>&nbsp;</p>
<h3><a name="Contact"></a>Contact</h3>
<p>Damir Cavar &lt;<a href="mailto:dcavar@indiana.edu?subject=BOOT-LA: ">dcavar@indiana.edu</a>&gt;</p>
<p>Indiana University<br>
Linguistics Dept.<br>
Memorial Hall, Room 402<br>
1021 E. Third Street<br>
Bloomington, IN. 47405-7005<br>
phone: (812) 855-3268<br>
fax: (812) 855-5363</p>
<p>&nbsp;</p>
<p align="right"><a href="#Top">back to TOP</a></p>
<p>&nbsp;</p>
<h3><a name="Program"></a>Program</h3>
<table border="1" cellpadding="0" cellspacing="0" style="border-collapse: collapse" bordercolor="#111111" width="749" id="AutoNumber2">
  <tr>
    <td width="249" align="left" valign="top"><a href="#Isabelle Barriere">
    <font face="Times New Roman" size="2">Isabelle Barriere, </font>
    <font size="2"><span style="font-family: Times New Roman">Marjorie Lorch</span></font></a></td>
    <td width="249" align="left" valign="top">
    <span style="font-family: Times New Roman"><font size="2">
    <a href="#Isabelle Barriere">Morphological bootstrapping in the acquisition 
    of Argument Structure</a></font></span></td>
    <td width="249" align="left" valign="top">&nbsp;</td>
  </tr>
  <tr>
    <td width="249" align="left" valign="top">
    <p class="MsoNormal"><font face="Times New Roman" size="2">
    <a href="#Aleka Blackwell">Aleka A. Blackwell</a></font></td>
    <td width="249" align="left" valign="top">
    <p class="MsoBodyText"><font face="Times New Roman" size="2">
    <a href="#Aleka Blackwell">How children’s developing theory of mind and the 
    semantic properties of lexical categories might account for patterns in the 
    acquisition of the English adjective class</a></font></td>
    <td width="249" align="left" valign="top">&nbsp;</td>
  </tr>
  <tr>
    <td width="249" align="left" valign="top">
    <font face="Times New Roman" size="2"><a href="#Morten Christiansen">Morten 
    Christiansen</a></font></td>
    <td width="249" align="left" valign="top">
    <font SIZE="2" face="Times New Roman"><a href="#Morten Christiansen">
    Syntactic Bootstrapping through Multiple-Cue Integration</a></font></td>
    <td width="249" align="left" valign="top">&nbsp;</td>
  </tr>
  <tr>
    <td width="249" align="left" valign="top">
    <p class="MsoNormal"><font face="Times New Roman" size="2">
    <a href="#Hamid Ekbia">Hamid R. Ekbia, Joshua Goldberg,<sup> </sup>David 
    Landy</a></font></td>
    <td width="249" align="left" valign="top">
    <p class="MsoNormal"><font face="Times New Roman" size="2">
    <a href="#Hamid Ekbia">Starting Large or Small? An Unresolved Dilemma for 
    Connectionist Models of Language Learning</a></font></td>
    <td width="249" align="left" valign="top">&nbsp;</td>
  </tr>
  <tr>
    <td width="249" align="left" valign="top">
    <font size="2" face="Times New Roman"><a name="Michael Gasser" href="#Gasser">
    Michael Gasser</a></font></td>
    <td width="249" align="left" valign="top">
    <font size="2" face="Times New Roman"><a href="#Gasser">tba</a></font></td>
    <td width="249" align="left" valign="top">&nbsp;</td>
  </tr>
  <tr>
    <td width="249" align="left" valign="top">
    <span style="font-family: Times New Roman"><font size="2">
    <a href="#Judith Gierut">Judith A. Gierut, Holly L. Storkel, Michele L. 
    Morrisette</a></font></span></td>
    <td width="249" align="left" valign="top">
    <span style="font-family: Times New Roman"><font size="2">
    <a href="#Judith Gierut">The syllable as a phonological bootstrap: 
    Linguistic categorization in typical and delayed development</a></font></span></td>
    <td width="249" align="left" valign="top">&nbsp;</td>
  </tr>
  <tr>
    <td width="249" align="left" valign="top">
    <font size="2" face="Times New Roman"><a href="#Annette Hohenberger">Annette 
    Hohenberger</a></font></td>
    <td width="249" align="left" valign="top">
    <p class="MsoNormal"><font face="Times New Roman" size="2">
    <span lang="EN-GB"><a href="#Annette Hohenberger">Bootstrapping into 
    recursive structures: Compounds, SCs, and phrasal syntax</a></span></font></td>
    <td width="249" align="left" valign="top">&nbsp;</td>
  </tr>
  <tr>
    <td width="249" align="left" valign="top">
    <font FACE="Times New Roman" size="2">
    <p ALIGN="LEFT"><a href="#Gaja Jarosz">Gaja E. Jarosz</a></font></td>
    <td width="249" align="left" valign="top">
    <font FACE="Times New Roman" SIZE="2">
    <p ALIGN="LEFT"><a href="#Gaja Jarosz">Separating Structure and Category 
    Learning: A Model of Phrasal Categories</a></font></td>
    <td width="249" align="left" valign="top">&nbsp;</td>
  </tr>
  <tr>
    <td width="249" align="left" valign="top">
    <font face="Times New Roman" size="2"><a href="#Christopher Johnson">
    Christopher Johnson</a></font></td>
    <td width="249" align="left" valign="top">
    <font face="Times New Roman" size="2"><span style="font-family: Times">
    <a href="#Christopher Johnson">Bootstrapping and prototypes</a></span></font></td>
    <td width="249" align="left" valign="top">&nbsp;</td>
  </tr>
  <tr>
    <td width="249" align="left" valign="top">
    <span lang="NL" style="font-family: Times New Roman"><font size="2">
    <a href="#Jacqueline van Kampen">Jacqueline van Kampen</a></font></span></td>
    <td width="249" align="left" valign="top">
    <p class="MsoNormal"><font face="Times New Roman" size="2"><span lang="NL">
    <a href="#Jacqueline van Kampen">Bootstrapping syntactic categories</a></span></font></td>
    <td width="249" align="left" valign="top">&nbsp;</td>
  </tr>
  <tr>
    <td width="249" align="left" valign="top">
    <font size="2" face="Times New Roman"><a href="#Fatma Ketrez">Fatma Nihan 
    Ketrez</a></font></td>
    <td width="249" align="left" valign="top">
    <font face="Times New Roman" size="2"><a href="#Fatma Ketrez">Is it possible 
    to bootstrap any lexical category information from word order in a 
    flexible-word-order language?</a></font></td>
    <td width="249" align="left" valign="top">&nbsp;</td>
  </tr>
  <tr>
    <td width="249" align="left" valign="top">
    <font face="Times New Roman" size="2"><a href="#Sean McLennan">Sean McLennan</a></font></td>
    <td width="249" align="left" valign="top">
    <font FACE="Times New Roman" size="2">
    <p ALIGN="LEFT"><a href="#Sean McLennan">Schema Theorem in Language 
    Acquisition: A Rags to Riches Story</a></font></td>
    <td width="249" align="left" valign="top">&nbsp;</td>
  </tr>
  <tr>
    <td width="249" align="left" valign="top">
    <font face="Times New Roman" size="2"><a href="#William Sakas">William G. 
    Sakas</a></font></td>
    <td width="249" align="left" valign="top">
    <font face="Times New Roman" size="2"><a href="#William Sakas">tba</a></font></td>
    <td width="249" align="left" valign="top">&nbsp;</td>
  </tr>
  <tr>
    <td width="249" align="left" valign="top">
    <font SIZE="2" face="Times New Roman"><a href="#SmithColungaYoshida">Linda Smith, 
    Eliana Colunga Hanako Yoshida</a></font></td>
    <td width="249" align="left" valign="top">
    <font SIZE="2" face="Times New Roman"><a href="#SmithColungaYoshida">Statistical 
    Bootstrapping: Correlations between the lexicon and perception create 
    &quot;ontological&quot; kinds</a></font></td>
    <td width="249" align="left" valign="top">&nbsp;</td>
  </tr>
  <tr>
    <td width="249" align="left" valign="top">
    <span style="font-family: Times New Roman"><font size="2">
    <a href="#Melanie Soderstrom">Melanie Soderstrom, Amanda Seidl, Deborah 
    Kemler Nelson, Jim Morgan</a></font></span></td>
    <td width="249" align="left" valign="top">
    <span style="font-family: Times New Roman"><font size="2">
    <a href="#Melanie Soderstrom">Infants are sensitive to the prosodic contours 
    of phrases</a></font></span></td>
    <td width="249" align="left" valign="top">&nbsp;</td>
  </tr>
  <tr>
    <td width="249" align="left" valign="top">
    <font size="2" face="Times New Roman"><a href="#Juan Uriagereka">Juan Uriagereka</a></font></td>
    <td width="249" align="left" valign="top">
    <font size="2" face="Times New Roman"><a href="#Juan Uriagereka">Categorial Dimensions and Sub-case Conditions 
    in Syntactic Bootstrapping</a></font></td>
    <td width="249" align="left" valign="top">&nbsp;</td>
  </tr>
  <tr>
    <td width="249" align="left" valign="top">
    <font size="2" face="Times New Roman"><a href="#Jürgen Weissenborn">Jürgen 
    Weissenborn</a></font></td>
    <td width="249" align="left" valign="top">
    <font size="2" face="Times New Roman"><a href="#Jürgen Weissenborn">tba</a></font></td>
    <td width="249" align="left" valign="top">&nbsp;</td>
  </tr>
  <tr>
    <td width="249" align="left" valign="top">
    <font face="Times New Roman" size="2"><a href="#Charles Yang">Charles D. 
    Yang</a></font></td>
    <td width="249" align="left" valign="top">
    <font face="Times New Roman" size="2"><a href="#Charles Yang">tba</a></font></td>
    <td width="249" align="left" valign="top">&nbsp;</td>
  </tr>
</table>
<p align="right"><a href="#Top">back to TOP</a></p>
<p>&nbsp;</p>
<h3><a name="Abstracts"></a>Abstracts</h3>
<p class="WPNormal">
<span style="font-family: Times New Roman; font-weight: 700">
<a name="SmithColungaYoshida"></a>Statistical Bootstrapping:&nbsp; Correlations 
between the lexicon and perception create &quot;ontological&quot; kinds</span></p>
<p class="WPNormal"><span style="font-family:&quot;Times New Roman&quot;">Linda 
Smith, Eliana Colunga and Hanako Yoshida (Indiana University)</span></p>
<p align="justify"><span style="font-size: 12.0pt; font-family: Times New Roman">Notions of 
animals, objects and substances are important to and reflected in the structure 
of language.&nbsp; In this work, we show how these notions are partly a product of 
language, constructed out of correlations between the lexicon, the perceptual 
properties of things in the world, and linguistic cues.&nbsp; We show how children’s 
learning of names for specific things creates abstract knowledge that transcends 
those experienced instances and also informs children’s subsequent judgments of 
novel instances, and their acquisition of new nouns. We use a connectionist 
network to show that these “rules” emerge as second-order generalizations over 
perceptual and linguistic regularities through ordinary processes of associative 
learning and generalization by similarity.&nbsp; And we, show that language learning 
matters by comparing children learning English and Japanese. </span></p>
<p>&nbsp;</p>
<p align="right"><a href="#Abstracts">back to TOP</a></p>
<p class="WPNormal"><b><a name="Gasser"></a>tba</b></p>
<p class="WPNormal"><span style="font-family: Times New Roman">Michael Gasser 
(Indiana University)</span></p>
<p align="justify">Alongside proposals that the language learner is bootstrapped in learning 
through regularities within the language itself or in the non-linguistic world 
are proposals that focus on constraints that originate in putative properties of 
the cognitive architecture and the interaction of this architecture with 
properties of the stimulus. In this paper I will discuss a proposal of this last 
type, a neural network model that starts with a simple form of competitive 
learning applied to the general problem of learning the mapping between forms 
and meanings. Given the largely arbitrary nature of the mapping, the model 
converges on mostly local representations of words, which in turn provide the 
basis for the fundamental symbolic properties of language: hierarchical 
representations and compositionality.<span style="font-size: 12.0pt; font-family: Times New Roman"> </span>
</p>
<p>&nbsp;</p>
<p align="right"><a href="#Abstracts">back to TOP</a></p>
<p>&nbsp;</p>
<h1> <font size="3"><a name="Juan Uriagereka"></a>Categorial Dimensions and Sub-case Conditions in Syntactic 
Bootstrapping</font></h1>
<p class="MsoNormal">Juan Uriagereka (University of Maryland and University of 
the Basque Country)</p>
<p align="justify">It is well-known that faced with 
an ambiguous situation which can be associated to a new lexical token <i>x</i>, 
such that <i>x</i> could denote either a boundless concept (e.g. an abstract or 
mass term, a state or an activity) or instead a concept which in some sense is 
bounded (e.g. a corresponding concrete count term, a telic event), children 
normally go for the second alternative. It is essentially only when terms 
denoting (perceptually ambiguous) concrete objects/events have already been 
acquired that children concentrate on more abstract objects/states, assuming 
that a new term <i>y</i> applied to the relevant percept cannot denote the very 
same notion that a distinct term <i>x </i>does. The question is how children 
achieve this result. How do they know to take a concrete interpretation when an 
unknown term is used to denote an (ambiguous) percept?</p>
<p align="justify">It may be thought that children 
inductively abstract from concrete instances, which could putatively explain the 
above state of affairs. However, that strategy leads to a rather different 
pattern. When faced with a new term <i>x</i> for an ambiguous percept, a 
seriously inductive learner ought to assume that <i>x</i> <i>names</i> a token 
percept of some sort, generalizing to some abstraction over other elements <i>
like</i> <i>x</i> only when discovering that a related, though different, token 
percept is also denoted as <i>x</i>. But in that sense, it ought to be equally 
easy for a child to generalize over token masses or states as it apparently is 
to generalize over token objects or events. That is, there is nothing a priori 
more or less natural about thinking of <i>x</i> as the name of a particular 
thing or event than it is to think of <i>x</i> as the name of a particular mass 
or state (unless we want to beg the question and claim that only events or 
things can be named, which is certainly false in merely logical terms). If 
anything, it should be easier to generalize over unstructured masses or states 
precisely because of their lack of structure; intuitively, it is easier to see 
that two token wines are that, wine, than it is to discover that two token 
animals are both dogs.</p>
<p align="justify">The (controversial) observation that mass terms are in some 
sense formally less complex than corresponding count terms is found, in some 
form, in Quine. It certainly squares well with the observation that in many 
languages counting requires additional grammatical formatives. The same 
conclusions (with similar grammatical evidence) can be reached for verbal 
elements, which leads to the major distinction between telic events and more 
basic elements in a Vendler-style hierarchy. Suppose, then, that on grammatical 
terms we find the basic hierarchical cuts (Abstract/mass Terms &lt; Count Elements) 
and (States/activities &lt; Telic Events). Does this have a bearing on the 
acquisition sequence?</p>
<p align="justify">On first examination, it would seem to work 
backwards: shouldn’t grammatically simpler terms be acquired before more complex 
ones? Well, not if the hierarchies above are innate, and what is acquired is 
merely an arbitrary term corresponding to them. Then it is actually reasonable 
for the acquirer to bet (in situations of ambiguous percepts) for a pairing with 
the more complex structuring first. That is the safest assumption to make, which 
could be corrected by positive data if it turned out to be wrong (the ‘Sub-case 
situation’). The opposite assumption is falsifiable only through negative 
evidence, which by hypothesis is unavailable to human learners prior to the 
acquisition process.</p>
<p>&nbsp;</p>
<p align="right"><a href="#Abstracts">back to TOP</a></p>
<p class="MsoNormal" align="center" style="text-align: left">&nbsp;</p>
<p class="MsoNormal" align="center" style="text-align: left"><b>
<a name="Hamid Ekbia"></a>Starting Large or Small? An Unresolved Dilemma for 
Connectionist Models of Language Learning</b></p>
<p class="MsoNormal" align="center" style="text-align: left">Hamid R. Ekbia, 
Joshua Goldberg and David Landy (Indiana University)</p>
<p class="MsoNormal" align="justify">The issue of negative evidence in language 
learning is at the core of the nativist-empiricist debate. Connectionist models 
of language learning have entered this debate from a constructivist perspective. 
The claim is that certain connectionist architectures can successfully learn 
simple grammars in the absence of negative evidence in the training set (Elman 
1990). What makes this possible, according to these accounts, is the mechanism 
of starting small. Roughly, this is the idea that the network, having started 
the learning process with simple inputs (with short-range dependencies), can 
bootstrap by gradually developing the right representations for processing more 
complex inputs.&nbsp; This is reportedly achieved by the manipulation of either the 
input data or the processing capacity of the network (e.g., the size of the 
input buffer or of the recurrent memory). On the other hand, attempts by other 
connectionists to replicate this experiment have either failed, or have led to 
rather contrary results (Rhode &amp; Plaut 1999), leaving an unresolved dilemma for 
connectionist research. The dilemma for connectionism would be how to model the 
learning of language by children. Is the model of Elman or that of Rhode and 
Plaut accurate to the situation faced by children? Or is neither relevantly 
accurate?</p>
<p class="MsoNormal" align="justify">Our purpose in this presentation is to 
argue that this dilemma cannot be resolved within the connectionist framework 
for a number of reasons — most notably:</p>
<ol style="margin-top: 0in; margin-bottom: 0in" start="1" type="1">
  <li class="MsoNormal">The impoverished notion of “semantics” embraced by 
  connectionist models of language learning</li>
  <li class="MsoNormal">The narrow notion of “context” embraced by 
  connectionists as something that only deals with the “linguistic” context </li>
  <li class="MsoNormal">The limited notion of “time” as, for instance, captured 
  in the recurrent connections of Elman nets</li>
</ol>
<p class="MsoNormal" align="justify">A thorough treatment of the debate requires 
revisiting the above notions in a fundamental manner that, we believe, would 
take one outside of the accepted principles of current connectionism.</p>
<p>&nbsp;</p>
<p align="right"><a href="#Abstracts">back to TOP</a></p>
<p class="MsoNormal" align="center" style="text-align: left">&nbsp;</p>
<p class="MsoNormal">
<span style="font-size: 12.0pt; font-family: Times New Roman; font-weight: 700">
<a name="Isabelle Barriere"></a>Morphological bootstrapping in the acquisition 
of Argument Structure</span></p>
<p class="MsoNormal">Isabelle Barriere (Johns Hopkins University &amp; University of 
Hertfordshire) and Marjorie Lorch (Applied Linguistics, Birkbeck College, 
London)</p>
<p class="MsoNormal" align="justify">The investigation of the acquisition of 
Argument Structure has led researchers to propose two bootstrapping hypotheses.&nbsp; 
According to the Semantic bootstrapping hypothesis (Pinker, 1989), children rely 
on semantic cues in order to predict Argument Structure Alternation.&nbsp; In 
contrast, according to the Syntactic bootstrapping hypothesis (Gleitman, 1990), 
children make use of the arguments in order to acquire Argument Structure 
Alternation.</p>
<p class="MsoNormal" align="justify">The first part of this paper demonstrates 
that these two hypotheses fail to consider the role of valency-marking morphemes 
in the process of acquisition of verb argument structure.&nbsp; In morphologically 
rich languages such as Inuktitut (Allen), Kiche (Pye, 1994) and Romance 
languages, the valency of the verbs is morphologically marked. However, when 
investigating the acquisition of these languages, most researchers have 
implicitly assumed that the thematic role assignment applied to the (sometimes 
ambiguous) morphological markers by children is appropriate (see Allen, 1996, 
among others).&nbsp; <span style="color: black">Thus children’s overgeneralizations 
have typically been classified into two categories: increased valency and 
decreased valency (Figueira, 1984; Allen, 1996). This classification fails to 
consider instances of unadult-like mapping between appropriate thematic role 
assignment and the inappropriate valency-marking morphological pattern 
associated with a verb which children have been shown to apply (see Allen, 1996 
for examples in Inuktitut and Pye, 1988 &amp; 1994 for Kiche).&nbsp; In light of this 
problem, we propose an alternative classification which includes an additional 
category -*Maintained Valency- which refers to instances when the thematic role 
assignment of the verb is maintained while the valency-marking morphological 
pattern associated with the verb is inappropriate.</span></p>
<p class="MsoNormal" align="justify"><span style="color: black">The 
consideration of these issues and the application of this new classification 
require the use of complementary research strategies when investigating the 
acquisition of AS.&nbsp; We illustrate our argument by presenting a study on ASA by 
French speaking children which relies on a range of methodological procedure 
(speech production, comprehension and grammaticality judgments, using real and 
nonce-verbs) and enable us to investigate children’s assignment of thematic 
roles to an ambiguous construction, namely the clitic SE+V which gives rise to 
reflexive, reciprocal, neuter/ergative (no implied agent) and middle-passive 
(implied agent) interpretations.&nbsp; The results obtained show that while children 
use semantic cues such as animacy in order to interpret these constructions, 
their representations do not match those of adults until at least age 5, as the 
middle-passive interpretation is not available to them.</span></p>
<p align="justify">
<span style="font-size: 12.0pt; font-family: Times New Roman; color: black">We 
conclude by discussing the need to consider ambiguous valency-marking morphemes 
in the acquisition of Argument Structure and by proposing a developmental 
account of children’s acquisition of SE-constructions which integrates semantic, 
syntactic and morphological cues.</span></p>
<p>&nbsp;</p>
<p align="right"><a href="#Abstracts">back to TOP</a></p>
<p class="MsoNormal" align="center" style="text-align: left">&nbsp;</p>
<p class="MsoBodyText"><b><a name="Aleka Blackwell"></a>How children’s 
developing theory of mind and the semantic properties of lexical categories 
might account for patterns in the acquisition of the English adjective class</b></p>
<p class="MsoBodyText">Aleka A. Blackwell (Middle Tennessee State University)</p>
<p class="MsoBodyText" align="justify"><span style="font-weight: normal">The 
issue of referential indeterminacy (Quine, 1960) has raised possibly the most 
important question in the study of word learning: How do children overcome this 
indeterminacy?&nbsp;&nbsp; Several word learning theories have appeared in the literature 
in recent years to account for children’s ability to learn the meaning of new 
words in what seems to be a very rapid rate.&nbsp; Addressing this issue, the present 
study presents evidence in support of the hypotheses that word learning is 
largely constrained by children’s development of a theory of mind (P. Bloom, 
2002) as well as by the properties of different semantic categories represented 
by a lexical class (Gasser &amp; Smith, 1998).&nbsp; The study analyzes the composition 
of the adjective lexicon in the language of two children ages 2;3–5;2 (Adam) and 
2;3–5;0 (Sarah) and their mothers (MacWhinney, 2000).&nbsp;&nbsp; The data are 7,449 child 
utterances and 6,437 maternal utterances with one of 305 adjectives. Child and 
maternal adjective use was examined in terms of six age categories (&lt;2;6, 
2;6–2;11, 3;0–3;5, 3;6–3;11, 4;0–4;5, 4;6–5;0).&nbsp; The semantic composition of the 
adjective lexicons of children and their mothers was analyzed in terms of 
Dixon’s (1982) semantic types: <span style="font-variant: small-caps">age, 
dimension, value, color, physical property, human propensity</span>.&nbsp; Adjective 
use by the children and their mothers was evaluated both in terms of type/token 
ratios for each semantic category—overall and at each developmental level—and in 
terms of frequencies of individual semantic types following Sandhofer et al. 
(2000).</span> <span style="font-weight: normal">The analyses indicate that 
mothers and children produce a somewhat similar number of adjective types 
(Sarah=127, Adam’s mother=138, Adam=143, Sarah’s mother=174), with Adam’s mother 
and Sarah producing larger numbers of tokens (Adam’s mother=4,402, Sarah=3,734, 
Sarah’s mother=3,047, Adam=2,703).&nbsp; However, proportionately,
<span style="font-variant: small-caps">color</span> adjectives, which represent 
10 to 16% of the adjective tokens in the two children’s speech respectively, are 
practically absent from the mother’s speech (0 and 1% respectively), and
<span style="font-variant: small-caps">value </span>and
<span style="font-variant: small-caps">human propensity</span> adjectives 
represent a larger proportion of the adjective tokens in adult speech whereas
<span style="font-variant: small-caps">physical property</span> adjectives 
represent a larger proportion of adjective tokens in child speech.&nbsp; The patterns 
also differ at each developmental stage with the children using more
<span style="font-variant: small-caps">color,</span></span><span style="font-variant: small-caps">
</span><span style="font-variant: small-caps; font-weight: normal">dimension</span><span style="font-weight: normal">, 
and <span style="font-variant: small-caps">physical property</span> adjectives 
and the mothers using more <span style="font-variant: small-caps">value</span> 
and <span style="font-variant: small-caps">human propensity</span> adjectives.&nbsp; 
I argue that the specific properties of the semantic categories to be learned by 
young children—even within one lexical class—play a significant role in 
children’s lexical development.</span><span style="font-weight: normal">&nbsp;</span></p>
<p class="MsoBodyText" align="left" style="text-align: left; line-height: 200%">
References</p>
<p class="MsoBodyText" align="left" style="text-align: left; text-indent: -9.0pt; margin-left: 9.0pt">
<span style="font-weight: normal">Bloom, P. (2002).&nbsp; <i>How children learn the 
meaning of words.&nbsp; </i>Cambridge</span><span style="font-weight: normal">,
</span><span style="font-weight: normal">MA: MIT Press.</span></p>
<p class="MsoBodyText" align="left" style="text-align: left; text-indent: -9.0pt; margin-left: 9.0pt">
<span style="font-weight: normal">Dixon, R.M.W. (1982).&nbsp; <i>Where have all the 
adjectives gone?: and other essays in semantics and syntax.</i> Berlin: Mouton.</span></p>
<p class="MsoBodyText" align="left" style="text-align: left; text-indent: -9.0pt; margin-left: 9.0pt">
<span style="font-weight: normal">Gasser, M. &amp; Smith, L.B. (1998).&nbsp; Learning 
nouns and adjectives: A connectionist account.&nbsp; <i>Language and Cognitive 
Processes, 13 (2/3)</i>, 269-306.</span></p>
<p class="MsoBodyText" align="left" style="text-align: left; text-indent: -9.0pt; margin-left: 9.0pt">
<span style="font-weight: normal">MacWhinney, B. (2000).&nbsp; <i>The CHILDES 
database: Tools for analyzing talk.&nbsp; 3<sup>rd</sup> Edition.&nbsp; Vol 2: The 
Database.</i>&nbsp; Mahwah</span><span style="font-weight: normal">, </span>
<span style="font-weight: normal">NJ: Lawrence Erlbaum Associates.</span></p>
<p class="MsoBodyText" align="left" style="text-align: left; text-indent: -9.0pt; margin-left: 9.0pt">
<span style="font-weight: normal">Quine, W.V.O. (1960).&nbsp; <i>Word and object.&nbsp;
</i>Cambridge</span><span style="font-weight: normal">, </span>
<span style="font-weight: normal">MA: MIT Press.</span></p>
<p class="MsoBodyText" align="left" style="text-align: left; text-indent: -9.0pt; margin-left: 9.0pt">
<span style="font-size: 12.0pt; font-family: Times New Roman">Sandhofer, C.M., 
Smith, L.B., &amp; Luo, J. (2000).&nbsp; Counting nouns and verbs in the input: 
differential frequencies, different kinds of learning?&nbsp; <i>Journal of Child 
Language 27</i>, 561-585.</span></p>
<p>&nbsp;</p>
<p align="right"><a href="#Abstracts">back to TOP</a></p>
<p class="MsoNormal" align="center" style="text-align: left">&nbsp;</p>
<p class="MsoBodyText"><b><font face="Times New Roman">
<a name="Morten Christiansen"></a>Syntactic Bootstrapping through Multiple-Cue 
Integration</font></b></p>
<p class="MsoBodyText"><font face="Times New Roman">Morten Christiansen (Cornell 
University)</font></p>
<p><font face="Times New Roman">tb</font>a</p>
<p>&nbsp;</p>
<p align="right"><a href="#Abstracts">back to TOP</a></p>
<p class="MsoNormal" align="center" style="text-align: left">&nbsp;</p>
<p class="MsoNormal" style="line-height: 150%"><b><a name="Judith Gierut"></a>
The syllable as a phonological bootstrap: Linguistic categorization in typical 
and delayed development</b></p>
<p class="MsoNormal" style="line-height: 150%">Judith A. Gierut (Indiana 
University), Holly L. Storkel (University of Kansas), Michele L. Morrisette 
(Indiana University)</p>
<p class="MsoNormal" align="justify">Phonological bootstrapping hypothesizes 
that language learners detect linguistically relevant phonological properties in 
the native language input, and then make use of these properties to organize 
their representation of that input.&nbsp; The syllable has been identified as one 
such phonological bootstrap, receiving robust support in both perception and 
production across a broad developmental span.&nbsp; Interestingly, however, few 
studies have directly examined what children know about the internal structural 
representation of syllables.&nbsp; In a series of 7 experiments, we address this 
question from dual perspectives of perception and production.&nbsp; Fifty preliterate 
preschool children, whose native language was English, were recruited and 
assigned to 1 of 2 groups: those with typically developing productive sound 
systems and those with delayed systems, as based on elicited productions.&nbsp; 
Children then participated in oddity tasks to determine their judgments of the 
perceived similarity of syllable structure, with particular emphasis on the 
onset.&nbsp; Four dimensions of structural similarity were manipulated across 
experiments, following from the purported linguistic representation of a 
syllable onset (cf. Clements &amp; Hume, 1995 for overview).&nbsp; These were (1) number 
of segments in onset position, (2) occurrence of onset-internal branching 
structure, (3) location of branching structure, and (4) conformity to the 
Sonority Sequencing Principle.&nbsp; In game format, the oddity task required a child 
to listen to unique legal triplets of syllables corresponding to the ‘names’ of 
identically pictured characters, and to judge which 2 of 3 were ‘friends’ (cf. 
Treiman &amp; Baron, 1981).&nbsp; Two main findings converged across experiments.&nbsp; First, 
the primary dimension that children used in judgments of the structural 
similarity of onsets was the Sonority Sequencing Principle and secondarily, the 
occurrence of branching.&nbsp; This suggests that children have access to details 
about subsyllabic structure that wholly accords with fully developed linguistic 
systems.&nbsp; Second, there were no differences in the similarity judgments of the 
two groups of children, despite obvious differences in their productive sound 
systems.&nbsp; These findings have implications for our understanding of the 
linguistic bases of categorization in development, and underscore long noted 
asymmetries in the relationship between perception and production.&nbsp; These 
asymmetries cannot be adequately handled by ‘spotlighting hypotheses’ associated 
with some phonological bootstrapping models (Peters &amp; Strömqvist, 1996), but are 
more parsimonious with contemporary constraint-based accounts of phonological 
development (Smolensky, 1996). [Supported by NIDCD 01694, NIDCD 04781]</p>
<p class="MsoNormal">References</p>
<p class="MsoNormal" style="text-indent: .5in">Clements, G. N., &amp; Hume, E. V. 
(1995). The internal organization of speech sounds. In J. A. Goldsmith (Ed.), <i>
The handbook of phonological theory</i> (pp. 245-306). Cambridge, MA: Blackwell.</p>
<p class="MsoNormal" style="text-indent: .5in">Peters, A. M., &amp; Strömqvist, S. 
(1996). The role of prosody in the acquisition of grammatical morphemes. In J. 
L. Morgan &amp; K. Demuth (Eds.), <i>Signal to syntax: Bootstrapping from speech to 
grammar in early acquisition</i> (pp. 215-232). Mahwah, NJ: Erlbaum. </p>
<p class="MsoNormal" style="text-indent: .5in">Smolensky, P. (1996). On the 
comprehension/production dilemma in child language. <i>Linguistic Inquiry, 27</i>, 
720-731.</p>
<p class="MsoNormal" style="text-indent: .5in">Treiman, R., &amp; Baron, J. (1981). 
Segmental analysis ability: Development and relation to reading ability. In G. 
E. MacKinnon &amp; T. G. Waller (Eds.), <i>Reading research: Advances in theory and 
practice</i> (pp. 159-198). New York: Academic Press.</p>
<p>&nbsp;</p>
<p align="right"><a href="#Abstracts">back to TOP</a></p>
<p class="MsoNormal" align="center" style="text-align: left">&nbsp;</p>
<p>
<span lang="EN-GB" style="font-size: 12.0pt; font-family: Times New Roman; font-weight: 700">
<a name="Annette Hohenberger"></a>Bootstrapping into recursive structures: 
Compounds, SCs, and phrasal syntax</span></p>
<p><span style="font-family: Times New Roman" lang="en-gb">Annette Hohenberger (Universität 
Frankfurt a.M.)</span></p>
<p class="MsoNormal" align="justify"><span lang="EN-GB">Recursion is the major 
characteristic of syntax. In this paper, I want to show by which bootstrapping 
processes the child eventually attains fully recursive syntax.</span></p>
<p class="MsoNormal" align="justify"><span lang="EN-GB">The lexicon as the 
earliest available linguistic working regime is a starting point for syntactic 
bootstrapping processes and also the fall-back option for not yet fully 
accomplished syntactic operations. Compounds are ideal candidates to investigate 
this liminal stage. They are morphologically complex though syntactically 
simplex. They are an early means to express propositional content within a 
single word. </span></p>
<p class="MsoNormal" align="justify"><span lang="EN-GB">The monolingual German 
child T uttering compounds such as (1a+2a) expresses the same proposition 
shortly later as a small clause (SC) (1b) or a complex NP (2b) (# indicating 
morpheme boundaries):</span></p>
<p class="MsoNormal"><span lang="DE">(1) (a)&nbsp; [sand#mund]<sub>N</sub> &nbsp; T 
(2;1)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (1) (b)&nbsp; [sand [im mund ]<sub> PP</sub>]<sub> SC </sub>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;T (2;1)</span></p>
<p class="MsoNormal"><span lang="DE">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span lang="EN-GB">’sand 
mouth’&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ’sand in-the mouth’</span></p>
<p class="MsoNormal"><span lang="EN-GB">&nbsp;</span></p>
<p class="MsoNormal"><span lang="DE">(2) (b) raesse [papa#buch]<sub>N</sub> &nbsp;&nbsp;&nbsp;&nbsp; 
T (2;5)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (2) (b)&nbsp; [grosse buch [vom papa ]<sub> PP</sub>]<sub> 
NP&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </sub>T (2;6)</span></p>
<p class="MsoNormal"><span lang="DE">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span lang="EN-GB">’big 
daddy book’&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‘big book from daddy’</span></p>
<p class="MsoNormal"><span lang="EN-GB">(Hohenberger 2002)</span></p>
<p class="MsoNormal" align="justify"><span lang="EN-GB">Compounds are attractive 
bootstraps because they only require embedding within the same module – the 
lexicon. Syntax, on the other hand, requires the joint cooperation of two 
modules – lexicon and syntax. In compounds, the child initiates crucial 
processes needed for full recursion, namely embedding through merger, but not 
yet move.</span></p>
<p class="MsoNormal" align="justify"><span lang="EN-GB">Full recursion is 
attained when the child also acquires Functional Categories (FCs) and masters 
syntactic movement and phrasal embedding. Through a whole series of 
bootstrapping processes the child’s grammar bifurcates into two modules – 
lexicon and syntax, with a clearly defined division of labour between them.</span></p>
<p class="MsoNormal" align="justify"><span lang="EN-GB">Once the child has 
started ‘putting words together’, her operations are restricted by interface 
constraints, most notably Phonetic Form (PF-) constraints regulating the proper 
serialization of elements in a linear string (Kayne 1994, Uriagereka 1998, Moro 
2000). The recursive nature of binary branching phrase structures thus reflects 
the basic task of grammar: mapping non-linear thoughts onto a strictly serial 
output channel. &nbsp;</span></p>
<p class="MsoNormal" align="justify"><span lang="EN-GB">Hohenberger, Annette 
(2002): Functional categories in language acquisition: Self-organization of a 
dynamical system. <i>Linguistische Arbeiten 456</i>. Tuebingen: Niemeyer.</span></p>
<p class="MsoNormal"><span lang="EN-GB">Kayne, R. (1994): <i>The antisymmetry of 
syntax</i>. Cambridge, Mass.: MIT Press.</span></p>
<p class="MsoNormal"><span lang="IT">Moro, A. (2000): Dynamical antisymmetry<i>.
</i></span><i><span lang="EN-GB">Linguistic Inquiry Monograph</span></i><span lang="EN-GB"> 
38. Cambridge, Mass.: MIT Press. </span></p>
<p class="MsoNormal"><span lang="EN-GB">Uriagereka, J. (1998): <i>Rhyme and 
reason: An introduction to minimalist syntax</i>. Cambridge, Mass.: MIT Press.</span></p>
<p>&nbsp;</p>
<p align="right"><a href="#Abstracts">back to TOP</a></p>
<p class="MsoNormal" align="center" style="text-align: left">&nbsp;</p>
<font FACE="Times New Roman">
<p ALIGN="LEFT"><b><a name="Gaja Jarosz"></a>Separating Structure and Category 
Learning: A Model of Phrasal Categories</b></p>
<p ALIGN="LEFT">Gaja E. Jarosz (Johns Hopkins University)</p>
</font><font FACE="Times New Roman" SIZE="3">
<p ALIGN="justify">Standard approaches to grammar induction employ probabilistic 
context-free grammars, and extensions thereof, usually in combination with the 
Inside-Outside algorithm [1, 2, 3]. Such research has not typically been 
concerned with the process by which children acquire grammatical knowledge. 
However, research in grammar acquisition supports a fundamentally different 
model, which is further supported by the limited success of the standard 
computational approaches.</p>
<p ALIGN="justify">The acquisition process may be divided into two separate 
processes: a structure learning component and a category learning component. The 
prosodic bootstrapping hypothesis states that children rely on various prosodic 
cues, such as intonation, pauses, syllable length, and pitch to identify 
syntactic constituents [4]. On the other hand, learning the categories of 
phrases depends on phonological, distributional, semantic, and syntactic cues 
[5]. In contrast, the grammars used in grammar induction do not make this 
distinction; they assign probabilities to the structure, or bracketing, and 
category labels of parse trees simultaneously.</p>
<p ALIGN="justify">This work describes a probabilistic grammar, RPCFG, which 
models the phrase structure categories component of grammar. Whereas a PCFG 
assigns a probability to the parse of a sentence, RPCFG (.Reverse. PCFG) assigns 
a probability to a labeling of a tree, given the structure. Specifically, RPCFG 
assigns a probability Pr(P→X Y | X Y) to a parent label P, given the structure 
and labels of its children: X Y. The probability of the tree labeling is the 
product of the probabilities of the individual rules. </p>
<p ALIGN="justify">To compare the performance of RPCFG and PCFG, the CKY parsing 
algorithm was adapted to search only the correct structures. Grammars trained on 
100, 1,000, and 10,000 sentences from a binarized version of the Wall Street 
Journal Treebank were tested on 1,000 sentences. Accuracy of labeling is 
determined by the number of correct labels divided by the total number of 
labels. On the largest training size, the grammars performed comparably: the 
PCFG achieved 80.0% accuracy and the RPCFG achieved 79.7%. However, on the 
smaller training sizes, RPCFG significantly outperformed the PCFG. Trained on 
100 sentences the RPCFG achieved 68.7% accuracy, while the PCFG achieved only 
26.0%. This means that after training on only 100 sentences, the RPCFG reached 
85% of its best performance!</p>
<p ALIGN="justify">These results suggest that bootstrapping between a successful 
structure-inducing system, such as Klein and Manning.s Generative 
Constituent-Context Model [6], and a label-inducing system based on RPCFG may be 
a more cognitively plausible, computationally feasible, and successful grammar 
induction system.</p>
<p ALIGN="justify">1) Chen, Stanley F (1995). <i>Bayesian Grammar Induction for 
Language Modeling</i>. Proceedings of the 33rd Annual Meeting of the ACL, 
228-235.</p>
<p ALIGN="justify">2) Lari, K., and S. J. Young (1990). <i>The estimation of 
stochastic context-free grammars using the insideoutside algorithm</i>. Computer 
Speech and Language, 4:35-56.</p>
<p ALIGN="justify">3) Carroll, G., and E. Charniak. (1992). <i>Two experiments 
on learning probabilistic dependency grammars from corpora</i>. In C. Weir, S. 
Abney, R Grishman, and R. Weischedel, editiors, Working Notes of the Workshop on 
Statistically-Based NLP Techniques, 1-13. AAAI Press.</p>
<p ALIGN="justify">4) Gleitman, L., Gleitman, H., Landau, B., &amp; Wanner, E. 
(1988). <i>Where learning begins: Initial representations for language learning</i>. 
In F.J. Newmeyer (Ed.), Language: Psychological and biological aspects: Volume 
III. Linguistics: The Cambridge Survey, 150-193. New York: Cambridge University 
Press.</p>
<p ALIGN="justify">5) Cartwright, T. A., and M. R. Brent. (1997). <i>Syntactic 
categorization in early language acquisition: Formalizing the role of 
distributional analysis</i>. Cognition, 62, 121-170.</p>
<p ALIGN="justify">6) Klein, D., and C. Manning (2002). <i>A Generative 
Constituent-Context Model for Improved Grammar Induction. </i>al Meeting of the 
ACL, 128-135.</p>
</font>
<p>&nbsp;</p>
<p align="right"><a href="#Abstracts">back to TOP</a></p>
<p class="MsoNormal" align="center" style="text-align: left">&nbsp;</p>
<p class="MsoNormal">
<span style="font-size: 12.0pt; font-family: Times; font-weight: 700">
<a name="Christopher Johnson"></a>Bootstrapping and prototypes</span></p>
<p class="MsoNormal">Christopher Johnson (University of Chicago)</p>
<p class="MsoNormal" align="justify">Formal syntactic categories correlate 
significantly yet imperfectly with semantic&nbsp; categories (noun with object, 
subject with agent, etc.). Theories of “semantic bootstrapping” maintain that 
such correlations are innate “inductive bases” for learning about formal 
categories (Grimshaw 1981, Pinker 1984). Other approaches maintain that such 
imperfect correlations provide evidence for prototype structure in grammatical 
categories (Lakoff&nbsp; 1987). This talk evaluates the explanatory value of these 
two views in the context of language acquisition, and proposes a synthesis that 
combines advantages of both views.</p>
<p class="MsoNormal" align="justify">The bootstrapping problem (Pinker 1984) 
arises because children need to learn distributional facts about formal 
categories without having any direct way to identify instances of those 
categories in their input. In semantic bootstrapping, children solve this 
problem by using innate knowledge of implications such as “If an expression 
denotes an object, then it is a noun or NP” (Grimshaw 1981). This view provides 
a mechanism by which children acquire new knowledge from experience in a way 
that is consistent with the Subset Principle. However, it posits a type of 
innate cognitive category (e.g. N or NP) that is unnatural because it is not 
characterized by criteria for membership. This view also fails to explain the 
relation between innate form-meaning correlations and the instances of adult 
grammatical categories that do not conform to them. Prototype accounts suggest 
principles of extension relating canonical and non-canonical instances, but do 
not associate those principles with strategies for acquiring knowledge from 
experience.</p>
<p class="MsoNormal" align="justify">I propose a view in which children start 
acquiring formal categories by identifying the specific formal means used to 
encode innate semantic notions like ‘object’ and ‘agent’, which are assumed to 
be especially amenable to linguistic encoding for pragmatic reasons. There are 
no innate formal categories. Children begin to extend their proto-grammatical 
forms in principled ways from early exemplars, as in a prototype account. The 
main mechanism of extension is not comparison to a prototype, however, but 
identification of phenomena that <i>correlate with</i> instances of innate 
semantic categories and are therefore exemplified by the earliest pairings of 
grammatical form and meaning.&nbsp; For example, human agents are especially salient 
examples of causes; frequent encoding of agents by subjects will lead children 
to associate subjects with the notion ‘cause’ as well. In this view, innate and 
pragmatically-accessible semantic categories “bootstrap” less accessible 
semantic categories associated with grammatical forms--principles of extension 
are therefore also strategies for acquiring knowledge from experience.</p>
<h1><font size="3">References</font></h1>
<p class="refs"><span style="font-family: Times New Roman">Grimshaw, Jane. 1981. 
Form, function, and the language acquisition device. In C.L. Baker and J. J. 
McCarthy (eds.), The logical problem of language acquisition, 165-182. 
Cambridge, MA: The MIT Press.</span></p>
<p class="refs"><span style="font-family: Times New Roman">Lakoff, George. 1987. 
Women, fire, and dangerous things: What categories reveal about the mind. 
Chicago: University of Chicago Press.</span></p>
<p class="refs"><span style="font-family: Times New Roman">Pinker, Steven. 
1984.&nbsp; Language learnability and language development.&nbsp; Cambridge, MA: Harvard 
University Press.</span></p>
<p>&nbsp;</p>
<p align="right"><a href="#Abstracts">back to TOP</a></p>
<p class="MsoNormal" align="center" style="text-align: left">&nbsp;</p>
<p class="MsoNormal"><b><span lang="NL"><a name="Jacqueline van Kampen"></a>
Bootstrapping syntactic categories</span></b></p>
<p class="MsoNormal"><span lang="NL">Jacqueline van Kampen (UiL OTS, Utrecht 
University)</span></p>
<p class="MsoBodyText" style="line-height: normal" align="justify">
<span style="font-size: 12.0pt">If all grammar resides in the grammatical 
properties assigned to a lexical item (Borer 1984, Chomsky 1995, and of course 
Categorial Grammar) it is important to see how grammatical categories are 
identified and assigned in first language acquisition.</span></p>
<p class="MsoBodyText" style="line-height: normal" align="justify">
<span style="font-size: 12.0pt">This paper will show how two highly frequent 
cues, the &lt;+fin&gt; marking of predicates and the &lt;+D&gt; marking of arguments lead to 
four category assignments.</span></p>
<h2><span lang="EN-GB" style="font-style: normal; font-weight: 400">
<font size="3">(1)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; a.&nbsp; cue &lt;+D&gt;, new category &lt;+N&gt; and &lt;+free anaphors&gt;</font></span></h2>
<h2 style="text-indent: -.25in; margin-left: 46.5pt">
<span lang="EN-GB" style="font-style: normal; font-weight: 400"><font size="3">
b.</font><span style="font-style: normal; font-variant: normal; font-family: Times New Roman"><font size="3">&nbsp;&nbsp;&nbsp;
</font></span><font size="3">cue &lt;+I/+fin&gt;, new category &lt;+V&gt; and &lt;+ nominative&gt;</font></span></h2>
<h2 align="justify">
<span lang="EN-GB" style="font-style: normal; font-weight: 400"><font size="3">
These are plausibly the first grammatical learning steps in most grammars as 
will be argued for by an analysis of French and Dutch child language.</font></span></h2>
<p class="MsoBodyText" style="line-height: normal" align="justify">
<span style="font-size: 12.0pt">The &lt;+fin&gt; marking of predicates and the &lt;+D&gt; 
marking of arguments suggest the introduction of the universal &lt;+V&gt; and &lt;+N&gt; by 
a context sensitive learning step.</span></p>
<p class="MsoNormal" style="text-align: justify; page-break-after: avoid">(3)<span style="letter-spacing: -.15pt">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
category neutral X&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>
<span style="font-family: Symbol; letter-spacing: -.15pt">Þ</span><span style="letter-spacing: -.15pt">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
&lt;+V&gt; /&nbsp; I/&lt;+fin&gt;&nbsp; </span>
<span style="font-family: Symbol; letter-spacing: -.15pt">¾</span></p>
<p class="MsoNormal" style="text-align: justify; page-break-after: avoid">
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span style="letter-spacing: -.15pt">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; category neutral X </span>
<span style="font-family: Symbol; letter-spacing: -.15pt">Þ</span><span style="letter-spacing: -.15pt">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
&lt;+N&gt; /&nbsp; D&nbsp; </span><span style="font-family: Symbol; letter-spacing: -.15pt">¾</span><span style="letter-spacing: -.15pt">
</span></p>
<p class="MsoBodyText" style="line-height: normal" align="justify">
<span style="font-size: 12.0pt">This is an alternative to Pinker’s (1984) 
semantic bootstrapping for &lt;+N&gt; and &lt;+V&gt;, and parallel to the proposal by
<span style="letter-spacing: -.15pt">Buszkowski (1987)</span> in Categorial 
Grammar<span style="letter-spacing: -.15pt">. A further and almost immediate 
effect of &lt;+fin&gt; is the &lt;+nominative&gt; subject in Spec,I. A further and almost 
immediate effect of &lt;+D&gt; is the rise of discourse (free) anaphors.</span></span></p>
<p class="MsoBodyText" style="line-height: normal" align="justify">
<span style="font-size: 12.0pt">The triggering contexts &lt;+fin&gt; and &lt;+D&gt; are 
systematically construed with a category neutral item. They systematically 
signify, respectively, a naming intention for D+X or a characterizing intention 
fro I+X. They function as bootstrapping cues due to their high text frequency. 
The general perspective of this paper is that cues for category assignment 
follow from a highly repetitive language specific context. This suggests that:</span></p>
<p class="MsoBodyText" style="line-height: normal">
<span style="font-size: 12.0pt">(4)&nbsp;&nbsp;&nbsp;&nbsp; a. Cues become effective only if part of 
the context has already been identified.</span></p>
<p class="MsoBodyText" style="line-height: normal">
<span style="font-size: 12.0pt">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; b. Acquisition steps are ordered 
accordingly</span></p>
<p class="MsoBodyText" style="line-height: normal">
<span style="font-size: 12.0pt">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; c. Language types are shaped by the 
different possibilities of learning steps.</span></p>
<p class="MsoBodyText" style="line-height: normal"><i>
<span style="font-size: 12.0pt">References</span></i></p>
<p class="MsoNormal" style="text-align: justify; text-indent: -10.2pt; margin-left: 10.2pt">
<span style="letter-spacing: -.15pt">Borer, H. (1984) </span><i>Parametric 
Syntax: Case Studies in Semitic and Romance Languages</i>, Dordrecht: Foris.</p>
<p class="MsoNormal" style="text-align: justify; text-indent: -10.1pt; margin-left: 10.1pt">
<span style="letter-spacing: -.15pt">Buszkowski, W. (1987) ‘Discovery procedures 
for categorial grammars’, in: E. Klein and J. van Benthem (eds.) <i>Categories, 
Polymorphism and Unification</i>, University of Amsterdam.</span></p>
<p class="MsoNormal" style="text-align: justify; text-indent: -10.1pt; margin-left: 10.1pt">
<span style="letter-spacing: -.15pt">Buszkowski, W. and G. Penn (1990) 
‘Categorial grammars determined from linguistic data by unification’, in: <i>
Studia Logica</i> 49, 431-454.</span></p>
<p class="MsoNormal" style="text-align: justify; text-indent: -10.2pt; margin-left: 10.2pt">
<span style="letter-spacing: -.15pt">Chomsky, N. (1995)&nbsp; <i>The Minimalist 
Program</i>, </span><span style="letter-spacing: -.15pt">Cambridge</span><span style="letter-spacing: -.15pt"> 
Mass./London: MIT Press.</span></p>
<p>
<span style="font-size: 12.0pt; font-family: Times New Roman; letter-spacing: -.15pt">
Pinker, S. (1984) <i>Language Learnability and Language Development</i>, 
Cambridge Mass./London: Harvard University Press.</span></p>
<p>&nbsp;</p>
<p align="right"><a href="#Abstracts">back to TOP</a></p>
<p class="MsoNormal" align="center" style="text-align: left">&nbsp;</p>
<h2 align="center" style="text-align: left"><font size="3">
<a name="Fatma Ketrez"></a>Is it possible to bootstrap lexical category 
information from word order in a flexible-word-order language?</font></h2>
<p>Fatma Nihan Ketrez (University of Southern California, Los Angeles)</p>
<h5 style="text-indent: 0in; margin-bottom: 0pt" align="justify">
<span style="font-weight: 400"><font size="3">This study investigates whether or 
not there is any significant regularity in the noun and verb distributions in 
Turkish child-directed-speech, which may be helpful to a child in the 
acquisition of lexical categories.</font></span></h5>
<h5 style="text-align: justify; text-indent: 0in; margin-bottom: 0pt">
<font size="3"><span style="font-weight: 400">According to Maratsos &amp; Chalkey 
(1986), input language can provide learners with sufficient information with 
respect to lexical categories. Any word that appears between <i>the </i>and <i>
is</i> in <i>the __ is</i> is a noun, for example. Analyses done on 
English-speaking parents’ language provide evidence for this proposal by showing 
that words belonging to the same category are <i>clustered </i>together 
according to their preceding and following contexts (Cartwright &amp; Brent 1997; 
Redington et. al. 1998; Mintz et. al. 2002). This proposal and the evidence in 
the literature are based on English, which is a fixed-word-order language, when 
compared to languages such as Turkish. Present study looks at linear 
distribution of nouns and verbs in Turkish child-directed-speech in order to 
investigate the cross-linguistic plausibility of this proposal. Turkish is 
especially interesting and challenging for a distributional analysis not only 
because of its relatively flexible word-order pattern, which is considered 
nothing but a <i>puzzle</i> for young learners (Kuntay &amp; Slobin 1996), but also 
because of its pro-drop mechanism and the lack of function words such as 
auxiliaries and articles.</span></font></h5>
<h5 style="text-align: justify; text-indent: 0in; margin-bottom: 0pt">
<span style="font-weight: 400"><font size="3">The corpora consist of 15.000 
adult utterances directed to four children between the ages 1:0 and 2:0. A 
series of three analyses are conducted on each child’s corpora individually. The 
first analysis examines the target words in their immediate contexts. In the 
later phases, context sizes are increased to two and six words on each side of 
the target words. The results obtained in each phase are compared quantitatively 
and qualitatively. The quantitative results show that the distribution of verbs 
is regular and better than chance according to the immediate contexts (<i>t</i>(3)=3.87,
<i>p</i>&lt;.05). With an increase in the context size, their clustering gets even 
better. The results obtained from the verb analyses provide a cross-linguistic 
support for the proposal for the distributional approaches suggesting that the 
distributional properties can provide a learner with useful grammatical 
information even in a language with a flexible word-order pattern. In noun 
distribution, however, there is no significant regularity in any context (<i>t</i>(3)=0.90,
<i>p</i>=.430) and that is problematic for a distribution analysis that looks at 
words without taking into consideration other cues such as the morphological 
structure of words. The qualitative analyses further reveal that rich 
morphological structure plays a role in clustering.</font></span></h5>
<h6 align="center" style="text-align: left">&nbsp;</h6>
<h6 align="center" style="text-align: left">
<span style="font-weight: 400; text-decoration: none"><font size="3">References</font></span></h6>
<p class="MsoBodyText" style="text-align: justify; line-height: normal">
<span style="font-family: Times New Roman">Cartwright, M. R. &amp; Brent, M. (1997) 
Syntactic Categorization in early language acquisition: Formalizing the role of 
distributional analysis. <i>Cognition 63,</i> 121-170.</span></p>
<p class="MsoBodyText" style="text-align: justify; line-height: normal">
<span style="font-family: Times New Roman">Küntay, A. &amp; Slobin D. I. (1996) 
Listening to A Turkish Mother: Some Puzzles for&nbsp; Acquisition. <i>Social </i>
</span><i><span style="font-family: Times New Roman">Interaction, Social Context 
&amp; Language: Essays in Honor of Susan Ervin-Tripp</span></i><span style="font-family: Times New Roman">, 
Hillsdale: Lawrence Erlbaum Associates Publishers, 265-286.</span></p>
<p class="MsoBodyText" style="text-align: justify; line-height: normal">
<span style="font-family: Times New Roman">Mintz, T., E. Newport &amp; T. Bever. 
(2002) The distributional structure of grammatical categories in speech to young 
children. <i>Cognitive Science 26</i>. 393-424.</span></p>
<p class="MsoBodyText" style="text-align: justify; line-height: normal">
<span style="font-family: Times New Roman">Maratsos, M &amp; Chalkley, M. A. (1980) 
The internal language of children’s syntax: The ontogenesis and representation 
of syntactic categories. In Nelson (Ed.) <i>Children’s Language</i>, Vol: 2 NY: 
Gardner Press.</span></p>
<p class="MsoBodyText" style="text-align: justify; line-height: normal">
<span style="font-family: Times New Roman">Redington, M. Chater, N. &amp; Finch, S. 
(1998) Distributional information: a powerful cue for acquiring syntactic 
categories, <i>Cognitive Science</i>, 22, 425-469.</span></p>
<p>&nbsp;</p>
<p align="right"><a href="#Abstracts">back to TOP</a></p>
<p class="MsoNormal" align="center" style="text-align: left">&nbsp;</p>
<p><b><a name="Sean McLennan"></a>Schema Theorem in Language Acquisition: A Rags 
to Riches Story</b></p>
<p><font face="Times New Roman">Sean McLennan (Indiana University)</font></p>
<font FACE="Times New Roman">
<p ALIGN="justify">The &quot;Poverty of the Stimulus&quot; (POS) argument, which maintains 
that the input a child receives from the environment is not sufficient to infer 
a productive grammar (Miller and Chomsky, 1963), has been central to the strong 
view of Universal Grammar. However, increasing evidence from other disciplines 
like Cognitive Science, and Psychology suggest that the POS may not be valid. 
Statistical models that can learn tasks that they were previously thought 
incapable of (ex. Chalmers, 1990; Elman 1995). Moreover, it appears that 
acquisition may progress more formulaically than has been believed (Tomasello, 
2000).</p>
<p ALIGN="justify">Despite these findings, the intuitions of Linguists 
concerning the course of L1 acquisition has not significantly changed—perhaps 
because, although there may be new evidence, the nature of the input, 
observations of which originally gave rise to the POS, has not changed. Nor does 
it seem that those initial observations were mistaken. Where then, is the 
&quot;extra&quot; information hiding?</p>
<p ALIGN="justify">&quot;Schema Theorem&quot;, originally proposed to deal with genetic 
algorithms (Holland, 1975; Mitchell, 1996), may provide insight into the 
&quot;missing&quot; information. Simply stated, Schema Theorem claims that a process that 
operates on a population of tokens, simultaneously, implicitly, operates on 
every category that those tokens instantiate. Schema Theorem, as generalized and 
applied to language, highlights the &quot;wealth of the stimulus&quot; that has heretofore 
been overlooked.</p>
<p ALIGN="justify">Examining the language learning environment through the lens 
of Schema Theorem has two important benefits. First, it helps resolve the 
tension between theoretical linguistics and other disciplines on the question of 
language learnability by expanding our perspective on the quantity and quality 
of L1 input. Second, it reduces the burden on biology for an explanation of 
language learnability which is more consistent with what we know about genetics 
and the brain.</p>
<p ALIGN="justify">Chalmers, David. 1990. Syntactic Transformations on 
Distributed Representations. <i>Connection Science</i>, 2/1-2:53-62.</p>
<p ALIGN="justify">Elman, Jeffery. 1995. Language as a Dynamical System. In 
Robert Port and Timothy van</p>
<p ALIGN="justify">Gelder (eds.). <i>Mind as Motion: Explorations in the 
Dynamics of Cognition. </i>Cambridge, MA: MIT Press.</p>
<p ALIGN="justify">Holland, John. 1975. <i>Adaptation in Natural and Artificial 
Systems. </i>Ann Arbor, MI: University of Michigan Press.</p>
<p ALIGN="justify">Miller, G.A. and Noam Chomsky. 1963. Finitary Models of 
Language Users. In: R.D. Luce,</p>
<p ALIGN="justify">R.R. Bush, and E. Galanter (eds.). <i>Handbook of 
Mathematical Psychology, Vol. 2. </i>New York, NY: Wiley.</p>
<p ALIGN="justify">Mitchell, Melanie. 1996. <i>An Introduction to Genetic 
Algorithms</i>. Cambridge, MA: MIT Press.</p>
<p ALIGN="justify">Tomasello, M. 2000. First Steps Toward a Usage-Based Theory 
of Language Acquisition. <i>Cognitive Linguistics</i>, 11/1-2:61:82</p>
</font>
<p>&nbsp;</p>
<p align="right"><a href="#Abstracts">back to TOP</a></p>
<p class="MsoNormal" align="center" style="text-align: left">&nbsp;</p>
<p><b><a name="William Sakas"></a>tba</b></p>
<p><font face="Times New Roman">William G. Sakas (City University of New York)</font></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p align="right"><a href="#Abstracts">back to TOP</a></p>
<p class="MsoNormal" align="center" style="text-align: left">&nbsp;</p>
<p class="MsoNormal"><b><a name="Melanie Soderstrom"></a>Infants are sensitive 
to the prosodic contours of phrases</b></p>
<p class="MsoNormal">Melanie Soderstrom (Brown University), Amanda Seidl (Johns 
Hopkins University), Deborah Kemler Nelson (Swarthmore College), Jim Morgan 
(Brown University)</p>
<p class="HTMLBody" align="justify">
<span style="font-size: 12.0pt; font-family: Times New Roman">Prosodic cues in 
the speech stream may provide infants with important information about the 
structure of their language. Considerable evidence suggests that infants use 
prosodic cues to segment speech at clause boundaries (e.g. Nazzi et al., 2000), 
but whether and how infants use prosodic cues to phrase boundaries is less 
clear. Prosodic cues coincident with phrase boundaries are less reliable, and 
infants show variable sensitivity to these boundaries, depending upon the 
methodology used (cf. Jusczyk et al., 1992; Soderstrom et al., in press) as well 
as specific acoustic/syntactic properties of the stimuli (Soderstrom et al.; 
Gerken et al., 1994).</span></p>
<p class="HTMLBody" align="justify">
<span style="font-size: 12.0pt; font-family: Times New Roman">One reason for 
such variable results may concern whether infants pay attention only to prosodic 
boundaries (dispreferring potential parses that contain them), or to the 
prosodic contours of the phrases (preferring globally well-formed parses). The 
current study disentangles these possibilities by comparing infants&#39; recognition 
of target word sequences (“<i>children were crying</i>”) that constitute either 
two adjacent phrasal units (“As a result, [<i>children</i>] [<i>were crying</i>] 
afterwards”) or two adjacent phrasal fragments (“Later on, [sad <i>children</i>] 
[<i>were crying</i> salty tears]”). In both cases, the word sequence contains a 
major phrase boundary. However, only in the second case is the target sequence 
syntactically ill-formed. Measurements of pause and preboundary lengthening at 
the sequence-internal major phrase boundary showed no differences between the 
&quot;well-formed&quot; and &quot;ill-formed&quot; target sequences in either stimulus set.</span></p>
<p class="MsoNormal" align="justify"><font face="Times New Roman">Preliminary 
results with 25 infants revealed a significant preference for the test passage 
containing the well-formed target sequence over the passage containing the 
ill-formed sequence in one of two stimulus sets tested. It may be that contour 
cues were stronger in the well-formed sequence of the successful stimulus set 
than that of the unsuccessful set. Or it may be that boundary cues were stronger 
in the unsuccessful set, such that the sequence-internal boundary was more 
strongly heard in the well-formed sequence for that set, masking any contour 
effects. Acoustic analyses found similarities in pitch accent cues across the 
two stimulus sets. However, there was a short pause (48 ms) prior to the start 
of the well-formed target sequence of the successful set that was not present in 
the other stimuli, which might help to signal the beginning of a phrase. In sum, 
these results suggest infants are (at least sometimes) sensitive to the prosodic 
well-formedness of the contour of the word sequence, and not just to the major 
phrase boundary.</font></p>
<p class="MsoNormal"><font face="Times New Roman">References</font></p>
<p class="MsoNormal" style="text-indent: -.25in; margin-left: .25in" align="justify">
<font face="Times New Roman">Gerken, L.-A., Jusczyk, P.W., &amp; Mandel, D.R. 
(1994). When prosody fails to cue syntactic structure: Nine-month-olds’ 
sensitivity to phonological versus syntactic phrases. <u>Cognition, 51,</u> 
237-265.</font></p>
<p class="MsoNormal" style="text-indent: -.25in; margin-left: .25in" align="justify">
<font face="Times New Roman">Jusczyk, P.W., Hirsh-Pasek, K., Kemler Nelson, D., 
Kennedy, L., Woodward, A., &amp; Piwoz, J. (1992). Perception of acoustic correlates 
of major phrasal units by young infants. <u>Cognitive Psychology, 24,</u> 
252-293.</font></p>
<p class="MsoNormal" style="text-indent: -.25in; margin-left: .25in" align="justify">
<font face="Times New Roman">Nazzi, T., Kemler Nelson, D.G., Jusczyk, P.W., &amp; 
Jusczyk, A.M. (2000). Six-month-olds’ detection of clauses embedded in 
continuous speech: Effects of prosodic well-formedness. <u>Infancy, 1,</u> 
123-147.</font></p>
<p class="HTMLBody" style="text-indent: -18.7pt; text-autospace: ideograph-numeric ideograph-other; margin-left: 18.7pt" align="justify">
<span style="font-family: Times New Roman">Soderstrom, M., Seidl, A., Kemler 
Nelson, D.G., &amp; Jusczyk, P.W. (in press). The prosodic bootstrapping of phrases: 
Evidence from prelinguistic infants</span></p>
<p>&nbsp;</p>
<p align="right"><a href="#Abstracts">back to TOP</a></p>
<p class="MsoNormal" align="center" style="text-align: left">&nbsp;</p>
<p><b><font face="Times New Roman"><a name="Jürgen Weissenborn"></a>tba</font></b></p>
<p><font face="Times New Roman">Jürgen Weissenborn (Universität Potsdam)</font></p>
<p><font face="Times New Roman">tba</font></p>
<p>&nbsp;</p>
<p align="right"><a href="#Abstracts">back to TOP</a></p>
<p class="MsoNormal" align="center" style="text-align: left">&nbsp;</p>
<p><font face="Times New Roman"><b><a name="Charles Yang"></a>tba</b></font></p>
<p><font face="Times New Roman">Charles D. Yang</font></p>
<p><font face="Times New Roman">tba</font></p>
<p>&nbsp;</p>
<p align="right"><a href="#Abstracts">back to TOP</a></p>
<p class="MsoNormal" align="center" style="text-align: left">&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<h3><a name="LSRL33"></a>LSRL 33</h3>
<p><b>Following this workshop will be &quot;The 33rd Linguistic Symposium on Romance Languages&quot; (Indiana University, Bloomington). Details on the LSRL can be found here:</b></p>
<p><a href="http://www.indiana.edu/~lsrl33/">LSRL 33 homepage</a><br>
&nbsp;</p>
<p align="right"><a href="#Top">back to TOP</a></p>
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-21704386-2']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
</body>
</html>